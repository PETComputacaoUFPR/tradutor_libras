{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to train and test different SVM models, by changing their hyperparameters, in order to obtain the best Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model library\n",
    "from LibrasModel import LibrasModel, weighted_accuracy_score, weighted_accuracy_scorer\n",
    "\n",
    "# model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# loading data\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# other modules\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get base_dataset\n",
    "data_path = \"TrainTestData/train_data.pickle\"\n",
    "data = pickle.load(open(data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# just to have an idea of the maximum max_depth\n",
    "forest = RandomForestClassifier(n_jobs=-1)\n",
    "model = LibrasModel(forest, has_z=True)\n",
    "X = model.transform_data(np.array(data[\"features\"]))\n",
    "y = np.array(data[\"labels\"])\n",
    "forest.fit(X, y)\n",
    "print(max([estimator.tree_.max_depth for estimator in forest.estimators_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for first Grid Search\n",
    "param_grid  = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [5, 15, 20, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gd(base_model, has_z, data, param_grid):\n",
    "    model = LibrasModel(base_model, has_z=has_z)\n",
    "    X = np.array(data[\"features\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    X = model.transform_data(X)\n",
    "    gd = GridSearchCV(model.model, param_grid, scoring=weighted_accuracy_scorer, return_train_score=True, cv=5, n_jobs=-1)\n",
    "    gd.fit(X, y)\n",
    "\n",
    "    cvres = gd.cv_results_ \n",
    "    results = sorted(zip(cvres[\"mean_test_score\"], cvres[\"params\"]), reverse=True, key=lambda x: x[0])\n",
    "    for mean_score, params in results:\n",
    "        print(mean_score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9289823652229966 {'max_depth': 30, 'n_estimators': 200}\n",
      "0.9266420189188022 {'max_depth': 20, 'n_estimators': 100}\n",
      "0.9253166662104609 {'max_depth': 30, 'n_estimators': 100}\n",
      "0.9252308745089796 {'max_depth': 20, 'n_estimators': 200}\n",
      "0.9249231907372154 {'max_depth': 15, 'n_estimators': 200}\n",
      "0.9222831156150197 {'max_depth': 30, 'n_estimators': 50}\n",
      "0.9220849192920781 {'max_depth': 15, 'n_estimators': 100}\n",
      "0.9197196459837832 {'max_depth': 20, 'n_estimators': 50}\n",
      "0.9176015767839438 {'max_depth': 15, 'n_estimators': 50}\n",
      "0.8934837375941935 {'max_depth': 20, 'n_estimators': 10}\n",
      "0.892161945351314 {'max_depth': 30, 'n_estimators': 10}\n",
      "0.8789505419026369 {'max_depth': 15, 'n_estimators': 10}\n",
      "0.7036813482900719 {'max_depth': 5, 'n_estimators': 200}\n",
      "0.7030504776953531 {'max_depth': 5, 'n_estimators': 100}\n",
      "0.6854882146290712 {'max_depth': 5, 'n_estimators': 50}\n",
      "0.6318401742605649 {'max_depth': 5, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# with z\n",
    "apply_gd(RandomForestClassifier(n_jobs=-1), True, data, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9324179417698509 {'max_depth': 15, 'n_estimators': 200}\n",
      "0.9316945067023299 {'max_depth': 20, 'n_estimators': 200}\n",
      "0.9307709963634657 {'max_depth': 30, 'n_estimators': 100}\n",
      "0.9297356270971816 {'max_depth': 30, 'n_estimators': 200}\n",
      "0.929677795694371 {'max_depth': 20, 'n_estimators': 50}\n",
      "0.9296225134068165 {'max_depth': 15, 'n_estimators': 100}\n",
      "0.9284201681218077 {'max_depth': 20, 'n_estimators': 100}\n",
      "0.9276494415639915 {'max_depth': 30, 'n_estimators': 50}\n",
      "0.9252849909378323 {'max_depth': 15, 'n_estimators': 50}\n",
      "0.9032109347339583 {'max_depth': 30, 'n_estimators': 10}\n",
      "0.9021576185635769 {'max_depth': 20, 'n_estimators': 10}\n",
      "0.9015027594938999 {'max_depth': 15, 'n_estimators': 10}\n",
      "0.7013342979804594 {'max_depth': 5, 'n_estimators': 100}\n",
      "0.6983504874019644 {'max_depth': 5, 'n_estimators': 200}\n",
      "0.6974061777314733 {'max_depth': 5, 'n_estimators': 50}\n",
      "0.6536476802839821 {'max_depth': 5, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# without z\n",
    "apply_gd(RandomForestClassifier(n_jobs=-1), False, data, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best model used the highest values of n_estimators, we will test some new values. Also, the max depth varied alot in the top models, so we will continue testing all three options again.\n",
    "\n",
    "The models without z outperformed the ones with z, so we will only use models withou z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid  = {\n",
    "    \"n_estimators\": [100, 200, 350, 500],\n",
    "    \"max_depth\": [15, 20, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9359604317694554 {'max_depth': 20, 'n_estimators': 200}\n",
      "0.934211978748827 {'max_depth': 15, 'n_estimators': 200}\n",
      "0.9337513054137924 {'max_depth': 20, 'n_estimators': 500}\n",
      "0.9337483010551992 {'max_depth': 20, 'n_estimators': 100}\n",
      "0.9331444932627369 {'max_depth': 15, 'n_estimators': 500}\n",
      "0.9326830857030547 {'max_depth': 30, 'n_estimators': 200}\n",
      "0.9311045501360221 {'max_depth': 15, 'n_estimators': 350}\n",
      "0.9308738662024126 {'max_depth': 30, 'n_estimators': 350}\n",
      "0.9298276395182763 {'max_depth': 20, 'n_estimators': 350}\n",
      "0.9294303578986891 {'max_depth': 30, 'n_estimators': 500}\n",
      "0.9287505027497168 {'max_depth': 15, 'n_estimators': 100}\n",
      "0.9274604243378578 {'max_depth': 30, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# without z\n",
    "apply_gd(RandomForestClassifier(n_jobs=-1), False, data, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing performance in all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(base_model, data, has_z=False):\n",
    "    model = LibrasModel(base_model, has_z=has_z)\n",
    "    \n",
    "\n",
    "    X = np.array(data[\"features\"])\n",
    "    X_transformed = model.transform_data(X)\n",
    "    y = np.array(data[\"labels\"])\n",
    "    metrics = {\n",
    "        \"acc_w\": weighted_accuracy_score,\n",
    "        \"acc\": accuracy_score\n",
    "    }\n",
    "    model.fit(X, y)\n",
    "    acc_w = cross_val_score(model.model, X_transformed, y, scoring=weighted_accuracy_scorer, cv=5)\n",
    "    acc = cross_val_score(model.model, X_transformed, y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "    t = time()\n",
    "    model.predict(X)\n",
    "    t = time() - t\n",
    "\n",
    "    print(f\"Weighted Accuracy: {round(100 * np.mean(acc_w), 2)}%\")\n",
    "    print(f\"Accuracy: {round(100 * np.mean(acc), 2)}%\")\n",
    "    print(f\"Time per prediction: {1000 * t / len(y)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestClassifier(max_depth=20, n_estimators=200)\n",
    "model2 = RandomForestClassifier(max_depth=15, n_estimators=200)\n",
    "model3 = RandomForestClassifier(max_depth=20, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy: 93.11%\n",
      "Accuracy: 92.8%\n",
      "Time per prediction: 0.031893469136336755 ms\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy: 93.36%\n",
      "Accuracy: 92.91%\n",
      "Time per prediction: 0.031031514036244358 ms\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy: 93.28%\n",
      "Accuracy: 92.82%\n",
      "Time per prediction: 0.07406416638144131 ms\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model3, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest models achives good results, especially in the prediction time.\n",
    "\n",
    "Since the top 2 models have basically the same performance and it seems the choice of the best between them depends on the random number generator, we will consider both of them in testing. The thid model has much higher predicion time, so it will not be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
